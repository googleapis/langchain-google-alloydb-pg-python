{
 "nbformat": 4,
 "nbformat_minor": 0,
 "metadata": {
  "colab": {
   "provenance": []
  },
  "kernelspec": {
   "name": "python3",
   "display_name": "Python 3"
  },
  "language_info": {
   "name": "python"
  }
 },
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "upi2EY4L9ei3"
   },
   "outputs": [],
   "source": [
    "# Copyright 2024 Google LLC\n",
    "#\n",
    "# Licensed under the Apache License, Version 2.0 (the \"License\");\n",
    "# you may not use this file except in compliance with the License.\n",
    "# You may obtain a copy of the License at\n",
    "#\n",
    "#     https://www.apache.org/licenses/LICENSE-2.0\n",
    "#\n",
    "# Unless required by applicable law or agreed to in writing, software\n",
    "# distributed under the License is distributed on an \"AS IS\" BASIS,\n",
    "# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n",
    "# See the License for the specific language governing permissions and\n",
    "# limitations under the License."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "mbF2F2miAT4a"
   },
   "source": [
    "[![Open In Colab](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/github/googleapis/langchain-google-alloydb-pg-python/blob/main/samples/langchain_quick_start.ipynb)\n",
    "\n",
    "---\n",
    "# **Introduction**\n",
    "\n",
    "In this codelab, you'll learn how to create a powerful interactive generative AI application using Retrieval Augmented Generation powered by [AlloyDB for PostgreSQL](https://cloud.google.com/alloydb) and [LangChain](https://www.langchain.com/). We will be creating an application grounded in a [Netflix Movie dataset](https://www.kaggle.com/datasets/shivamb/netflix-shows), allowing you to interact with movie data in exciting new ways."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Ma6pEng3ypbA"
   },
   "source": [
    "## Prerequisites\n",
    "\n",
    "* A basic understanding of the Google Cloud Console\n",
    "* Basic skills in command line interface and Google Cloud shell\n",
    "* Basic python knowledge"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "DzDgqJHgysy1"
   },
   "source": [
    "## What you'll learn\n",
    "\n",
    "* How to deploy a AlloyDB for PostgreSQL instance\n",
    "* How to use AlloyDB for PostgreSQL as a VectorStore\n",
    "* How to use AlloyDB for PostgreSQL as a DocumentLoader\n",
    "* How to use AlloyDB for PostgreSQL for ChatHistory storage"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "FbcZUjT1yvTq"
   },
   "source": [
    "## What you'll need\n",
    "* A Google Cloud Account and Google Cloud Project\n",
    "* A web browser such as [Chrome](https://www.google.com/chrome/)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "vHdR4fF3vLWA"
   },
   "source": [
    "# **Setup and Requirements**\n",
    "\n",
    "In the following instructions you will learn to:\n",
    "1. Install required dependencies for our application\n",
    "2. Set up authentication for our project\n",
    "3. Set up a AlloyDB for PostgreSQL Instance\n",
    "4. Import the data used by our application"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "uy9KqgPQ4GBi"
   },
   "source": [
    "## Install dependencies\n",
    "First you will need to install the dependencies needed to run this demo app."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "M_ppDxYf4Gqs"
   },
   "outputs": [],
   "source": [
    "%pip install langchain_google_alloydb_pg\n",
    "\n",
    "%pip install langchain langchain-google-vertexai"
   ]
  },
  {
   "cell_type": "code",
   "source": [
    "%pip install \"google-cloud-alloydb-connector[pg8000]\""
   ],
   "metadata": {
    "id": "iXGq1naviLry"
   },
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "DeUbHclxw7_l"
   },
   "source": [
    "## Authenticate to Google Cloud within Colab\n",
    "In order to access your Google Cloud Project from this notebook, you will need to Authenticate as an IAM user."
   ]
  },
  {
   "cell_type": "code",
   "source": [
    "from google.colab import auth\n",
    "\n",
    "auth.authenticate_user()"
   ],
   "metadata": {
    "id": "_Q9hyqdyEx6l"
   },
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "UCiNGP1Qxd6x"
   },
   "source": [
    "## Connect Your Google Cloud Project\n",
    "Time to connect your Google Cloud Project to this notebook so that you can leverage Google Cloud from within Colab."
   ]
  },
  {
   "cell_type": "code",
   "source": [
    "# @markdown Please fill in the value below with your GCP project ID and then run the cell.\n",
    "\n",
    "# Please fill in these values.\n",
    "project_id = \"\"  # @param {type:\"string\"}\n",
    "\n",
    "# Quick input validations.\n",
    "assert project_id, \"⚠️ Please provide a Google Cloud project ID\"\n",
    "\n",
    "# Configure gcloud.\n",
    "!gcloud config set project {project_id}"
   ],
   "metadata": {
    "id": "SLUGlG6UE2CK"
   },
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "O-oqMC5Ox-ZM"
   },
   "source": [
    "## Enable APIs for AlloyDB and Vertex AI within your project"
   ]
  },
  {
   "cell_type": "markdown",
   "source": [
    "You will need to enable these APIs in order to create an AlloyDB database and utilize Vertex AI as an embeddings service!"
   ],
   "metadata": {
    "id": "X-bzfFb4A-xK"
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "CKWrwyfzyTwH"
   },
   "outputs": [],
   "source": [
    "# enable GCP services\n",
    "!gcloud services enable alloydb.googleapis.com\n",
    "!gcloud services enable aiplatform.googleapis.com"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Gn8g7-wCyZU6"
   },
   "source": [
    "## Set up AlloyDB\n",
    "You will need a **Postgres** AlloyDB instance for the following stages of this notebook. Please set the following variables."
   ]
  },
  {
   "cell_type": "code",
   "source": [
    "# @markdown Please fill in the both the Google Cloud region and name of your AlloyDB instance. Once filled in, run the cell.\n",
    "\n",
    "# Please fill in these values.\n",
    "region = \"\"  # @param {type:\"string\"}\n",
    "instance_name = \"\"  # @param {type:\"string\"}\n",
    "database_name = \"\"  # @param {type:\"string\"}\n",
    "password = input(\"Please provide a password to be used for 'postgres' database user: \")\n",
    "cluster_name = \"\"  # @param {type:\"string\"}"
   ],
   "metadata": {
    "id": "8q2lc-Po1mPv"
   },
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "T616pEOUygYQ"
   },
   "source": [
    "### Create an AlloyDB Instance\n",
    "If you have already created an AlloyDB Cluster and Instance, you can skip these steps and skip to the connectivity section."
   ]
  },
  {
   "cell_type": "markdown",
   "source": [
    "First let's create an AlloyDB Cluster.\n",
    "> ⏳ - Creating an AlloyDB cluster may take a few minutes.\n"
   ],
   "metadata": {
    "id": "xyZYX4Jo1vfh"
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "XXI1uUu3y8gc"
   },
   "outputs": [],
   "source": [
    "# Quick input validations.\n",
    "assert region, \"⚠️ Please provide a Google Cloud region\"\n",
    "assert instance_name, \"⚠️ Please provide the name of your instance\"\n",
    "assert database_name, \"⚠️ Please provide the name of your database_name\"\n",
    "\n",
    "#create the AlloyDB Cluster\n",
    "!gcloud beta alloydb clusters create {cluster_name} --password={password} --region={region}\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "source": [
    "Now that we have created our AlloyDB Cluster, we can create an instance attached to our cluster with the following command.\n",
    "> ⏳ - Creating an AlloyDB instance may take a few minutes."
   ],
   "metadata": {
    "id": "o8LkscYH5Vfp"
   }
  },
  {
   "cell_type": "code",
   "source": [
    "!gcloud beta alloydb instances create {instance_name} --instance-type=PRIMARY --cpu-count=2 --region={region} --cluster={cluster_name}"
   ],
   "metadata": {
    "id": "TkqQSWoY5Kab"
   },
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "source": [
    "In order to connect to your newly created AlloyDB instance from this notebook, you will need to enable public IP on your instance. Alternatively, you can follow [these instructions](https://cloud.google.com/alloydb/docs/connect-external) to connect to an AlloyDB for PostgreSQL instance with Private IP from outside your VPC."
   ],
   "metadata": {
    "id": "BXsQ1UJv4ZVJ"
   }
  },
  {
   "cell_type": "code",
   "source": [
    "!gcloud beta alloydb instances update {instance_name} --region={region} --cluster={cluster_name} --assign-inbound-public-ip=ASSIGN_IPV4"
   ],
   "metadata": {
    "id": "OPVWsQB04Yyl"
   },
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "source": [
    "Now let's set the connection string that we will use to connect to our instance."
   ],
   "metadata": {
    "id": "mjA6AiAzB2Du"
   }
  },
  {
   "cell_type": "code",
   "source": [
    "connection_string = \"projects/{0}/locations/{1}/clusters/{2}/instances/{3}\".format(\n",
    "    project_id, region, cluster_name, instance_name\n",
    ")\n",
    "print(connection_string)"
   ],
   "metadata": {
    "id": "acF-nMPGKJVQ"
   },
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": [
    "from google.cloud.alloydb.connector import Connector, IPTypes\n",
    "import sqlalchemy\n",
    "\n",
    "# initialize Connector object\n",
    "connector = Connector()\n",
    "\n",
    "\n",
    "# function to return the database connection\n",
    "def getconn():\n",
    "    conn = connector.connect(\n",
    "        connection_string,\n",
    "        \"pg8000\",\n",
    "        user=\"postgres\",\n",
    "        password=password,\n",
    "        db=\"postgres\",\n",
    "        enable_iam_auth=False,\n",
    "        ip_type=IPTypes.PUBLIC,\n",
    "    )\n",
    "    return conn\n",
    "\n",
    "\n",
    "# create connection pool\n",
    "pool = sqlalchemy.create_engine(\n",
    "    \"postgresql+pg8000://\", creator=getconn, isolation_level=\"AUTOCOMMIT\"\n",
    ")"
   ],
   "metadata": {
    "id": "zsLi0a7FIYjT"
   },
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "source": [
    "##Create a Database"
   ],
   "metadata": {
    "id": "i_yNN1MnJpTR"
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "Now that we can connect to our AlloyDB instance from this notebook, let's create a database we will use to store the data for this application. You may get an error that there is no public ip address, this is because assigning a public ip address takes a few minutes. Please wait and retry this step if you hit an error!"
   ],
   "metadata": {
    "id": "6xOPJoVsdOvZ"
   }
  },
  {
   "cell_type": "code",
   "source": [
    "create_db_cmd = sqlalchemy.text(\n",
    "    f\"CREATE DATABASE {database_name}\",\n",
    ")\n",
    "with pool.connect() as db_conn:\n",
    "    db_conn.execute(create_db_cmd)\n",
    "connector.close()"
   ],
   "metadata": {
    "id": "hPE6tt5eJqhq"
   },
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "HdolCWyatZmG"
   },
   "source": [
    "## Import data to your database\n",
    "\n",
    "Now that you have your database, you will need to import data! We will be using a [Netflix Dataset from Kaggle](https://www.kaggle.com/datasets/shivamb/netflix-shows). Here is what the data looks like:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "36-FBKzJ-tLa"
   },
   "source": [
    "| show_id | type    | title                | director         | cast                                                                                                                                                  | country       | date_added        | release_year | rating | duration  | listed_in                                    | description                                                                                                                                                                           |\n",
    "|---------|---------|----------------------|------------------|--------------------------------------------------------------------------------------------------------------------------------------------------------|---------------|-------------------|--------------|--------|-----------|----------------------------------------------|---------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------|\n",
    "| s1      | Movie   | Dick Johnson Is Dead | Kirsten Johnson  |                                                                                                                                                        | United States | September 25, 2021 | 2020         | PG-13  | 90 min    | Documentaries                                | As her father nears the end of his life, filmmaker Kirsten Johnson stages his death in inventive and comical ways to help them both face the inevitable.                              |\n",
    "| s2      | TV Show | Blood & Water        |                  | Ama Qamata, Khosi Ngema, Gail Mabalane, Thabang Molaba, Dillon Windvogel, Natasha Thahane, Arno Greeff, Xolile Tshabalala, Getmore Sithole, Cindy Mahlangu, Ryle De Morny, Greteli Fincham, Sello Maake Ka-Ncube, Odwa Gwanya, Mekaila Mathys, Sandi Schultz, Duane Williams, Shamilla Miller, Patrick Mofokeng | South Africa  | September 24, 2021 | 2021         | TV-MA  | 2 Seasons | International TV Shows, TV Dramas, TV Mysteries | After crossing paths at a party, a Cape Town teen sets out to prove whether a private-school swimming star is her sister who was abducted at birth.                                   |\n",
    "| s3      | TV Show | Ganglands            | Julien Leclercq  | Sami Bouajila, Tracy Gotoas, Samuel Jouy, Nabiha Akkari, Sofia Lesaffre, Salim Kechiouche, Noureddine Farihi, Geert Van Rampelberg, Bakary Diombera                                   |               | September 24, 2021 | 2021         | TV-MA  | 1 Season  | Crime TV Shows, International TV Shows, TV Action & Adventure | To protect his family from a powerful drug lord, skilled thief Mehdi and his expert team of robbers are pulled into a violent and deadly turf war.                                     |\n",
    "| s4      | TV Show | Jailbirds New Orleans |                  |                                                                                                                                                        |               | September 24, 2021 | 2021         | TV-MA  | 1 Season  | Docuseries, Reality TV                        | Feuds, flirtations and toilet talk go down among the incarcerated women at the Orleans Justice Center in New Orleans on this gritty reality series.                                   |\n",
    "| s5      | TV Show | Kota Factory         |                  | Mayur More, Jitendra Kumar, Ranjan Raj, Alam Khan, Ahsaas Channa, Revathi Pillai, Urvi Singh, Arun Kumar                                                 | India        | September 24, 2021 | 2021         | TV-MA  | 2 Seasons | International TV Shows, Romantic TV Shows, TV Comedies | In a city of coaching centers known to train India’s finest collegiate minds, an earnest but unexceptional student and his friends navigate campus life. |\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "kQ2KWsYI_Msa"
   },
   "source": [
    "Instead of leaving it to you to figure out how to insert these documents, we have prepared code to help you insert the csv into your AlloyDB for PostgreSQL database."
   ]
  },
  {
   "cell_type": "markdown",
   "source": [
    "First lets download the csv file and add it to our notebook."
   ],
   "metadata": {
    "id": "Dzr-2VZIkvtY"
   }
  },
  {
   "cell_type": "code",
   "source": [
    "!gsutil cp gs://cloud-samples-data/langchain/first_five_netflix_titles.csv ."
   ],
   "metadata": {
    "id": "5KkIQ2zSvQkN"
   },
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "source": [
    "We have downloaded the csv file to our home directory, to see it run ls. You should also be able to see the csv file populate in the \"Files\" tab."
   ],
   "metadata": {
    "id": "oFU13dCBlYHh"
   }
  },
  {
   "cell_type": "code",
   "source": [
    "!ls"
   ],
   "metadata": {
    "id": "nQBs10I8vShh"
   },
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "source": [
    "Lets add a connection function to connect to our db using the AlloyDB sychronous connector."
   ],
   "metadata": {
    "id": "BotJ5x-8DBhQ"
   }
  },
  {
   "cell_type": "code",
   "source": [
    "from google.cloud.alloydb.connector import Connector, IPTypes\n",
    "import sqlalchemy\n",
    "\n",
    "# initialize Connector object\n",
    "connector = Connector()\n",
    "\n",
    "\n",
    "# function to return the database connection\n",
    "def getconn():\n",
    "    conn = connector.connect(\n",
    "        connection_string,\n",
    "        \"pg8000\",\n",
    "        user=\"postgres\",\n",
    "        password=password,\n",
    "        db=database_name,\n",
    "        enable_iam_auth=False,\n",
    "        ip_type=IPTypes.PUBLIC,\n",
    "    )\n",
    "    return conn\n",
    "\n",
    "\n",
    "# create connection pool\n",
    "pool = sqlalchemy.create_engine(\n",
    "    \"postgresql+pg8000://\", creator=getconn, isolation_level=\"AUTOCOMMIT\"\n",
    ")"
   ],
   "metadata": {
    "id": "n88BbOvDlXtx"
   },
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "source": [
    "In this next step we will\n",
    "1. Create the table into which we will insert the data.\n",
    "2. Map over the columns of our csv file to the columns of our datatable and insert the data!"
   ],
   "metadata": {
    "id": "2H7rorG9Ivur"
   }
  },
  {
   "cell_type": "code",
   "source": [
    "import pandas as pd\n",
    "\n",
    "create_table_cmd = sqlalchemy.text(\n",
    "    'CREATE TABLE netflix_titles ( \\\n",
    "    show_id VARCHAR, \\\n",
    "    type VARCHAR, \\\n",
    "    title VARCHAR, \\\n",
    "    director VARCHAR, \\\n",
    "    \"cast\" VARCHAR, \\\n",
    "    country VARCHAR, \\\n",
    "    date_added VARCHAR, \\\n",
    "    release_year INTEGER, \\\n",
    "    rating VARCHAR, \\\n",
    "    duration VARCHAR, \\\n",
    "    listed_in VARCHAR, \\\n",
    "    description TEXT \\\n",
    "    )',\n",
    ")\n",
    "\n",
    "netflix_data = \"/content/first_five_netflix_titles.csv\"\n",
    "\n",
    "df = pd.read_csv(netflix_data)\n",
    "insert_data_cmd = sqlalchemy.text(\n",
    "    \"\"\"\n",
    "    INSERT INTO netflix_titles VALUES (:show_id, :type, :title, :director,\n",
    "      :cast, :country, :date_added, :release_year, :rating,\n",
    "      :duration, :listed_in, :description)\n",
    "    \"\"\"\n",
    ")\n",
    "\n",
    "parameter_map = [\n",
    "    {\n",
    "        \"show_id\": row[\"show_id\"],\n",
    "        \"type\": row[\"type\"],\n",
    "        \"title\": row[\"title\"],\n",
    "        \"director\": row[\"director\"],\n",
    "        \"cast\": row[\"cast\"],\n",
    "        \"country\": row[\"country\"],\n",
    "        \"date_added\": row[\"date_added\"],\n",
    "        \"release_year\": row[\"release_year\"],\n",
    "        \"rating\": row[\"rating\"],\n",
    "        \"duration\": row[\"duration\"],\n",
    "        \"listed_in\": row[\"listed_in\"],\n",
    "        \"description\": row[\"description\"],\n",
    "    }\n",
    "    for index, row in df.iterrows()\n",
    "]\n",
    "\n",
    "with pool.connect() as db_conn:\n",
    "    db_conn.execute(create_table_cmd)\n",
    "    db_conn.execute(\n",
    "        insert_data_cmd,\n",
    "        parameter_map,\n",
    "    )\n",
    "    db_conn.commit()\n",
    "connector.close()"
   ],
   "metadata": {
    "id": "qCsM2KXbdYiv"
   },
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "SsGS80H04bDN"
   },
   "source": [
    "# **Use case 1: AlloyDB for Postgres as a document loader**\n",
    "\n",
    "---\n",
    "\n",
    "\n",
    "\n",
    "Now that you have data in your database, you are ready to use AlloyDB for PostgreSQL as a document loader. This means we will pull data from the database and load it into memory as documents. We can then feed these documents into the vector store."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "-CQgPON8dwSK"
   },
   "source": [
    "Next let's connect to our AlloyDB for PostgreSQL instance using the AlloyDBEngine class."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "zrwTsWHMkQ_v"
   },
   "outputs": [],
   "source": [
    "from langchain_google_alloydb_pg import AlloyDBEngine, Column, AlloyDBLoader\n",
    "\n",
    "engine = AlloyDBEngine.from_instance(\n",
    "    project_id=project_id,\n",
    "    instance=instance_name,\n",
    "    region=region,\n",
    "    cluster=cluster_name,\n",
    "    database=database_name,\n",
    "    user=\"postgres\",\n",
    "    password=password,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "8s-C0P-Oee69"
   },
   "source": [
    "Once we initialize an AlloyDBEngine object, we can pass it into the AlloyDBSQLLoader to connect to a specific database. As you can see we also pass in a query, table_name and a list of columns. The query tells the loader what query to use to pull data. The \"content_columns\" argument refers to the columns that will be used as \"content\" in the document object we will later construct. The rest of the columns in that table will become the \"metadata\" associated with the documents."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "2SdFJT6Vece1"
   },
   "outputs": [],
   "source": [
    "table_name = \"netflix_titles\"\n",
    "content_columns = [\"title\", \"director\", \"cast\", \"description\"]\n",
    "loader = await AlloyDBLoader.create(\n",
    "    engine=engine,\n",
    "    query=f\"SELECT * FROM {table_name};\",\n",
    "    content_columns=content_columns,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "xvCEAp97fXRt"
   },
   "source": [
    "Next let's define a function \"collect_async_items\" to asynchronously pull documents from our database."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "-p9UiHFbdyYa"
   },
   "outputs": [],
   "source": [
    "async def collect_async_items(docs_generator):\n",
    "    \"\"\"Collects items from an async generator.\"\"\"\n",
    "    docs = []\n",
    "    async for doc in docs_generator:\n",
    "        docs.append(doc)\n",
    "    return docs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "dsL-KFrmfuS1"
   },
   "source": [
    "Then let's run the function to pull our documents from out database using our document loader. You can see the first 5 documents from the database here. Nice, you just used AlloyDB Postgres as a document loader!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "t4zTx-HLfwmW"
   },
   "outputs": [],
   "source": [
    "documents = await collect_async_items(loader.alazy_load())\n",
    "print(f\"Loaded {len(documents)} from the database. 5 Examples:\")\n",
    "for doc in documents[:5]:\n",
    "    print(doc)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "z9uLV3bs4noo"
   },
   "source": [
    "# **Use case 2: AlloyDB for PostgreSQL as Vector Store**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "duVsSeMcgEWl"
   },
   "source": [
    "Now, let's learn how to put all of the documents we just loaded into a vector store so that we can use vector search to answer our user's questions!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "jfH8oQJ945Ko"
   },
   "source": [
    "### Create Your Vector Store table\n",
    "\n",
    "Based on the documents that we loaded before, we want to create a table with a vector column as our vector store. We will start it by intializing a vector table by calling the `init_vectorstore_table` function from our `engine`. As you can see we list all of the columns for our metadata. We also specify a vector size, 768, that corresponds with the length of the vectors computed by the model our embeddings service uses, Vertex AI's textembedding-gecko.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "e_rmjywG47pv"
   },
   "outputs": [],
   "source": [
    "from langchain_google_alloydb_pg import AlloyDBEngine, Column\n",
    "\n",
    "sample_vector_table_name = \"movie_vector_table_samples\"\n",
    "\n",
    "engine = AlloyDBEngine.from_instance(\n",
    "    project_id=project_id,\n",
    "    instance=instance_name,\n",
    "    region=region,\n",
    "    cluster=cluster_name,\n",
    "    database=database_name,\n",
    "    user=\"postgres\",\n",
    "    password=password,\n",
    ")\n",
    "\n",
    "engine.init_vectorstore_table(\n",
    "    sample_vector_table_name,\n",
    "    vector_size=768,\n",
    "    metadata_columns=[\n",
    "        Column(\"show_id\", \"VARCHAR\", nullable=True),\n",
    "        Column(\"type\", \"VARCHAR\", nullable=True),\n",
    "        Column(\"country\", \"VARCHAR\", nullable=True),\n",
    "        Column(\"date_added\", \"VARCHAR\", nullable=True),\n",
    "        Column(\"release_year\", \"INTEGER\", nullable=True),\n",
    "        Column(\"rating\", \"VARCHAR\", nullable=True),\n",
    "        Column(\"duration\", \"VARCHAR\", nullable=True),\n",
    "        Column(\"listed_in\", \"VARCHAR\", nullable=True),\n",
    "    ],\n",
    "    overwrite_existing=True,  # Enabling this will recreate the table if exists.\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "KG6rwEuJLNIo"
   },
   "source": [
    "### Try inserting the documents into the vector table\n",
    "\n",
    "Now we will create a vector_store object backed by our vector table in the AlloyDB database. Let's load the data from the documents to the vector table. Note that for each row, the embedding service will be called to compute the embeddings to store in the vector table. Pricing details can be found [here](https://cloud.google.com/vertex-ai/pricing)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Wo4-7EYCIFF9"
   },
   "outputs": [],
   "source": [
    "from langchain_google_vertexai import VertexAIEmbeddings\n",
    "from langchain_google_alloydb_pg import AlloyDBVectorStore, AlloyDBEngine\n",
    "\n",
    "# Initialize the embedding service. In this case we are using version 003 of Vertex AI's textembedding-gecko model. In general, it is good practice to specify the model version used.\n",
    "embeddings_service = VertexAIEmbeddings(\n",
    "    model_name=\"textembedding-gecko@003\", project=project_id\n",
    ")\n",
    "\n",
    "engine = AlloyDBEngine.from_instance(\n",
    "    project_id=project_id,\n",
    "    instance=instance_name,\n",
    "    region=region,\n",
    "    cluster=cluster_name,\n",
    "    database=database_name,\n",
    "    user=\"postgres\",\n",
    "    password=password,\n",
    ")\n",
    "\n",
    "vector_store = AlloyDBVectorStore.create_sync(\n",
    "    engine=engine,\n",
    "    embedding_service=embeddings_service,\n",
    "    table_name=sample_vector_table_name,\n",
    "    metadata_columns=[\n",
    "        \"show_id\",\n",
    "        \"type\",\n",
    "        \"country\",\n",
    "        \"date_added\",\n",
    "        \"release_year\",\n",
    "        \"duration\",\n",
    "        \"listed_in\",\n",
    "    ],\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "fr1rP6KQ-8ag"
   },
   "source": [
    "Now let's try to put the documents data into the vector table. Here is a code example to load the first 5 documents in the list."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "CTks8Cy--93B"
   },
   "outputs": [],
   "source": [
    "import uuid\n",
    "\n",
    "docs_to_load = documents[:5]\n",
    "\n",
    "# ! Uncomment the following line to load all 8,800+ documents to the database vector table with calling the embedding service.\n",
    "# docs_to_load = documents\n",
    "\n",
    "ids = [str(uuid.uuid4()) for i in range(len(docs_to_load))]\n",
    "vector_store.add_documents(docs_to_load, ids)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "29iztdvfL2BN"
   },
   "source": [
    "### Import the rest of your data into your vector table\n",
    "\n",
    "You don't have to call the embedding service 8,800 times to load all the documents for the demo. Instead, we have prepared a csv with the all 8,800+ rows with pre-computed embeddings in a csv file. Again, let's import the csv using gsutil."
   ]
  },
  {
   "cell_type": "code",
   "source": [
    "!gsutil cp gs://cloud-samples-data/langchain/alloydb/netflix_titles_embeddings.csv ."
   ],
   "metadata": {
    "id": "4dE4oQiNyWdC"
   },
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "source": [
    "And now let's insert the csv data into the table containing our vectors."
   ],
   "metadata": {
    "id": "T1TXnKU_DznX"
   }
  },
  {
   "cell_type": "code",
   "source": [
    "netflix_data = \"/content/langchain_alloydb_netflix_computed_embeddings.csv\"\n",
    "df = pd.read_csv(netflix_data)\n",
    "insert_data_cmd = sqlalchemy.text(\n",
    "    \"\"\"\n",
    "    INSERT INTO movie_vector_table_samples VALUES (:langchain_id, :content, :embedding, :show_id,\n",
    "      :type, :country, :date_added, :release_year, :rating,\n",
    "      :duration, :listed_in, :langchain_metadata)\n",
    "    \"\"\"\n",
    ")\n",
    "\n",
    "parameter_map = [\n",
    "    {\n",
    "        \"langchain_id\": row[\"langchain_id\"],\n",
    "        \"content\": row[\"content\"],\n",
    "        \"embedding\": row[\"embedding\"],\n",
    "        \"show_id\": row[\"show_id\"],\n",
    "        \"type\": row[\"type\"],\n",
    "        \"country\": row[\"country\"],\n",
    "        \"date_added\": row[\"date_added\"],\n",
    "        \"release_year\": row[\"release_year\"],\n",
    "        \"rating\": row[\"rating\"],\n",
    "        \"duration\": row[\"duration\"],\n",
    "        \"listed_in\": row[\"listed_in\"],\n",
    "        \"langchain_metadata\": row[\"langchain_metadata\"],\n",
    "    }\n",
    "    for index, row in df.iterrows()\n",
    "]\n",
    "\n",
    "with pool.connect() as db_conn:\n",
    "    db_conn.execute(\n",
    "        insert_data_cmd,\n",
    "        parameter_map,\n",
    "    )\n",
    "    db_conn.commit()\n",
    "connector.close()"
   ],
   "metadata": {
    "id": "PDIRYfUyyVyI"
   },
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ZM_OFzZrQEPs"
   },
   "source": [
    "# **Use case 3: AlloyDB for PostgreSQL as Chat Memory**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "dxqIPQtjDquk"
   },
   "source": [
    "Next we will add chat history (called “memory” in the context of LangChain) to our application so the LLM can retain context and information across multiple interactions, leading to more coherent and sophisticated conversations or text generation. We can use AlloyDB for PostgreSQL as “memory” storage in our application so that the LLM can use context from prior conversations to better answer the user’s prompts. First let's initialize AlloyDB for PostgreSQL as memory storage."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "vyYQILyoEAqg"
   },
   "outputs": [],
   "source": [
    "from langchain_google_alloydb_pg import AlloyDBChatMessageHistory, AlloyDBEngine\n",
    "\n",
    "engine = AlloyDBEngine.from_instance(\n",
    "    project_id=project_id,\n",
    "    instance=instance_name,\n",
    "    region=region,\n",
    "    cluster=cluster_name,\n",
    "    database=database_name,\n",
    "    user=\"postgres\",\n",
    "    password=password,\n",
    ")\n",
    "message_table_name = \"message_store\"\n",
    "\n",
    "engine.init_chat_history_table(table_name=message_table_name)\n",
    "\n",
    "chat_history = AlloyDBChatMessageHistory.create_sync(\n",
    "    engine,\n",
    "    session_id=\"my-test-session\",\n",
    "    table_name=message_table_name,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "2yuXYLTCl2K1"
   },
   "source": [
    "Here is an example of how you would add a user message and how you would add an AI message."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "qDVoTWZal0ZF"
   },
   "outputs": [],
   "source": [
    "chat_history.add_user_message(\"Hi!\")\n",
    "chat_history.add_ai_message(\"Hello there. I'm a model and am happy to help!\")\n",
    "\n",
    "chat_history.messages"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "k0O9mta8RQ0v"
   },
   "source": [
    "# **Conversational RAG Chain backed by AlloyDB**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "j2OxF3JoNA7J"
   },
   "source": [
    "So far we've tested out using AlloyDB for PostgreSQL as document loader, Vector Store and Chat Memory. Now let's use it in the `ConversationalRetrievalChain`.\n",
    "\n",
    "We will build a chat bot that can answer movie related questions based on the vector search results.\n",
    "\n",
    "First let's initialize all of our AlloyDBEngine object to use as a connection in our vector store and chat_history."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "9ukjOO-sNQ8_"
   },
   "outputs": [],
   "source": [
    "from langchain_google_vertexai import VertexAIEmbeddings, VertexAI\n",
    "from langchain_core.messages import AIMessage, HumanMessage\n",
    "from langchain.chains import ConversationalRetrievalChain\n",
    "from langchain.memory import ConversationSummaryBufferMemory\n",
    "from langchain_core.prompts import PromptTemplate\n",
    "from langchain_google_alloydb_pg import (\n",
    "    AlloyDBEngine,\n",
    "    AlloyDBVectorStore,\n",
    "    AlloyDBChatMessageHistory,\n",
    ")\n",
    "\n",
    "# Intialize the embedding service\n",
    "embeddings_service = VertexAIEmbeddings(\n",
    "    model_name=\"textembedding-gecko@latest\", project=project_id\n",
    ")\n",
    "\n",
    "# Intialize the engine\n",
    "engine = AlloyDBEngine.from_instance(\n",
    "    project_id=project_id,\n",
    "    instance=instance_name,\n",
    "    region=region,\n",
    "    cluster=cluster_name,\n",
    "    database=database_name,\n",
    "    user=\"postgres\",\n",
    "    password=password,\n",
    ")\n",
    "# Intialize the Vector Store\n",
    "vector_store = AlloyDBVectorStore.create_sync(\n",
    "    engine=engine,\n",
    "    embedding_service=embeddings_service,\n",
    "    table_name=sample_vector_table_name,\n",
    "    metadata_columns=[\n",
    "        \"show_id\",\n",
    "        \"type\",\n",
    "        \"country\",\n",
    "        \"date_added\",\n",
    "        \"release_year\",\n",
    "        \"duration\",\n",
    "        \"listed_in\",\n",
    "    ],\n",
    ")\n",
    "\n",
    "# Intialize the AlloyDBChatMessageHistory\n",
    "chat_history = AlloyDBChatMessageHistory.create_sync(\n",
    "    engine,\n",
    "    session_id=\"my-test-session\",\n",
    "    table_name=\"message_store\",\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Ytlz9D3LmcU7"
   },
   "source": [
    "Let's create a prompt for the LLM. Here we can add instructions specific to our application, such as \"Don't make things up\"."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "LoAHNdrWmW9W"
   },
   "outputs": [],
   "source": [
    "# Prepare some prompt templates for the ConversationalRetrievalChain\n",
    "prompt = PromptTemplate(\n",
    "    template=\"\"\"Use all the information from the context and the conversation history to answer new question. If you see the answer in previous conversation history or the context. \\\n",
    "Answer it with clarifying the source information. If you don't see it in the context or the chat history, just say you \\\n",
    "didn't find the answer in the given data. Don't make things up.\n",
    "\n",
    "Previous conversation history from the questioner. \"Human\" was the user who's asking the new question. \"Assistant\" was you as the assistant:\n",
    "```{chat_history}\n",
    "```\n",
    "\n",
    "Vector search result of the new question:\n",
    "```{context}\n",
    "```\n",
    "\n",
    "New Question:\n",
    "```{question}```\n",
    "\n",
    "Answer:\"\"\",\n",
    "    input_variables=[\"context\", \"question\", \"chat_history\"],\n",
    ")\n",
    "condense_question_prompt_passthrough = PromptTemplate(\n",
    "    template=\"\"\"Repeat the following question:\n",
    "{question}\n",
    "\"\"\",\n",
    "    input_variables=[\"question\"],\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "rsGe-bW5m0H1"
   },
   "source": [
    "Now let's use our vector store as a retreiver. Retreiver's in Langchain allow us to literally \"retrieve\" documents."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "1nI0xkJamvXt"
   },
   "outputs": [],
   "source": [
    "# Intialize retriever, llm and memory for the chain\n",
    "retriever = vector_store.as_retriever(\n",
    "    search_type=\"mmr\", search_kwargs={\"k\": 5, \"lambda_mult\": 0.8}\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "3maZ8SLlneYJ"
   },
   "source": [
    "Now let's intialize our LLM, in this case we are using Vertex AI's \"gemini-pro\"."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "VBWhg-ihnnxF"
   },
   "outputs": [],
   "source": [
    "llm = VertexAI(model_name=\"gemini-pro\", project=project_id)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "hN8mpXdtnocg"
   },
   "source": [
    "We clear our chat history, so that our application starts without any prior context to other conversations we have had with the application."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "1UkPcEpJno5Y"
   },
   "outputs": [],
   "source": [
    "chat_history.clear()\n",
    "\n",
    "memory = ConversationSummaryBufferMemory(\n",
    "    llm=llm,\n",
    "    chat_memory=chat_history,\n",
    "    output_key=\"answer\",\n",
    "    memory_key=\"chat_history\",\n",
    "    return_messages=True,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "BDAT2koSn8Mz"
   },
   "source": [
    "Now let's create a conversational retrieval chain. This will allow the LLM to use chat history in it's responses, meaning we can ask it follow up questions to our questions instead of having to start from scratch after each inquiry."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "7Fu8fKdEn8h8"
   },
   "outputs": [],
   "source": [
    "# create the ConversationalRetrievalChain\n",
    "rag_chain = ConversationalRetrievalChain.from_llm(\n",
    "    llm=llm,\n",
    "    retriever=retriever,\n",
    "    verbose=False,\n",
    "    memory=memory,\n",
    "    condense_question_prompt=condense_question_prompt_passthrough,\n",
    "    combine_docs_chain_kwargs={\"prompt\": prompt},\n",
    ")\n",
    "\n",
    "# ask some questions\n",
    "q = \"What movie was Brad Pitt in?\"\n",
    "ans = rag_chain({\"question\": q, \"chat_history\": chat_history})[\"answer\"]\n",
    "print(f\"Question: {q}\\nAnswer: {ans}\\n\")\n",
    "\n",
    "q = \"How about Jonny Depp?\"\n",
    "ans = rag_chain({\"question\": q, \"chat_history\": chat_history})[\"answer\"]\n",
    "print(f\"Question: {q}\\nAnswer: {ans}\\n\")\n",
    "\n",
    "q = \"Are there movies about animals?\"\n",
    "ans = rag_chain({\"question\": q, \"chat_history\": chat_history})[\"answer\"]\n",
    "print(f\"Question: {q}\\nAnswer: {ans}\\n\")\n",
    "\n",
    "# browser the chat history\n",
    "chat_history.messages"
   ]
  }
 ]
}
